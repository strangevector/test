spark.conf.set("spark.sql.extensions=org.apache.iceberg.spark. extensions. IcebergSparkSessionExtension")
spark. conf. set("spark.sql. catalog.glue_catalog=org.apache.iceberg.spark. SparkCatalog")
spark.conf.set("spark.sql.catalog.glue_catalog.warehouse =< S3-location-for-icebergtable>")
spark.conf.set("spark.sql.catalog.glue_catalog.catalog-impl=org.apache.iceberg.aws.glue. GlueCatalog")
spark.conf.set("spark.sql. catalog.glue_catalog.io-impl=org.apache.iceberg.aws.s3.S3FileI0")

from pyspark.sql import SparkSession
from pyspark.sql.functions import when, to_timestamp, concat, to_utc_timestamp, from_utc_timestamp, date_format, collect_set, array_max, array_min, lit, col
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, ArrayType, FloatType, TimestampType, DateType
spark = SparkSession.builder.getOrCreate()

from datetime import datetime, timedelta
import random as rd
from string import ascii_lowercase as alphas
from pyspark.sql.functions import year, month, date_of_month

schema = StructType([
    StructField("account_id", IntegerType(), True),
    StructField("bill", FloatType(), True),
    StructField("timestamp", TimestampType(), True),
    StructField("category", StringType(), True),
])
dids = {}
rdf = spark.createDataFrame([], schema=schema)
# attendance.createOrReplaceTempView("attendanceT")

def stream(d, ids, duf):
    data = []
    tts = {}
    for i in ids:
        tts[i] = datetime.fromisoformat(f'{d[0]:04}-{d[1]:02}-{rd.randint(1, 7):02}')
        if i not in dids:
            dids[i] = round(rd.uniform(10.0, 100.0), 2)
    for i in ids:
        tts[i] += timedelta(hours=rd.randint(3, 4), minutes=rd.randint(1, 30), seconds=rd.randint(1, 30)) 
        dids[i] = dids[i] + rd.choice([0, round(rd.uniform(5.0, 10.0), 2)])
        data.append((i, dids[i], tts[i], alphas[i-1]*3))
    for _ in range(int(len(ids)*(duf-1))):
        i = rd.choice(ids)
        tts[i] += timedelta(days=rd.randint(5, 7), hours=rd.randint(3, 4), minutes=rd.randint(1, 30), seconds=rd.randint(1, 30))
        dids[i] = dids[i] + rd.choice([0, round(rd.uniform(5.0, 10.0), 2)])
        data.append((i, dids[i], tts[i], alphas[i-1]*3))
    return spark.createDataFrame(data, schema)

def updaterdf(rdf, streamdf):
    for d in streamdf.collect():
        si = rdf.where(rdf["account_id"] == d.account_id)
        if si.count() == 0:
            rdf = rdf.union(spark.createDataFrame([d], schema))
        else:
            siy = si.where(year(si['timestamp']) == year(lit(d.timestamp)))
            if siy.count() == 0:
                rdf = rdf.union(spark.createDataFrame([d], schema))
            else:
                siym = siy.where(month(siy['timestamp']) == month(lit(d.timestamp)))
                if siym.count() == 0:
                    rdf = rdf.union(spark.createDataFrame([d], schema))
                elif siym.where(rdf['timestamp'] < lit(d.timestamp)):
                    mask = (rdf["account_id"] == lit(d.account_id)) & (year(rdf['timestamp']) == year(lit(d.timestamp))) & (month(rdf['timestamp']) == month(lit(d.timestamp)))
                    rdf = rdf.withColumn("bill", when(mask, d.bill).otherwise(rdf["bill"])) \
                            .withColumn("timestamp", when(mask, d.timestamp).otherwise(rdf["timestamp"]))
    return rdf

d24_1 = stream((2024, 1), range(1, 4), 1.7)
d24_1.show()

d24_2 = stream((2024, 2), range(3, 6), 1.5)
d24_2.show()
