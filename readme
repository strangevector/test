%idle_timeout 60
%glue_version 4.0
%worker_type G.1X
%number_of_workers 2
%%configure
{
"--conf":"spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions" # brings support for Iceberg tables
    " --conf spark.sql.catalog.glue_catalog=org.apache.iceberg.spark.SparkCatalog" # catalog used to manage Iceberg tables
    " --conf spark.sql.catalog.glue_catalog.warehouse=s3://xxxxxx/prime/warehouse/" # location where tables and metadata will be stored
    " --conf spark.sql.catalog.glue_catalog.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog" # implementation class for the catalog (AWS Glue Catalog integration)
    " --conf spark.sql.catalog.glue_catalog.io-impl=org.apache.iceberg.aws.s3.S3FileIO" # Iceberg will use the S3FileIO implementation for reading and writing data to Amazon S3
    " --conf spark.sql.catalog.glue_catalog.glue.lakeformation-enabled=true" # Lake Formation integration for the GlueCatalog
    " --conf spark.sql.catalog.glue_catalog.glue.id=xxxxxx" # AWS account ID associated with the GlueCatalog. It uniquely identifies the catalog,
"--datalake-formats":"iceberg"
}
# "--packages": "org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.0.0,org.apache.iceberg:iceberg-aws-bundle:1.0.0"

from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.utils import getResolvedOptions
from pyspark.sql import SparkSession
catalog_nm = "glue_catalog"
s3_bucket = "s3://xxxxxx/prime/warehouse/"
sc = spark.sparkContext
glueContext = GlueContext(sc)
job = Job(glueContext)

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, TimestampType
import pyspark.sql.functions as f
from datetime import datetime, timedelta
import random as rd
from string import ascii_lowercase as alphas

def stream(d, ids, dupf=0):
    data = []
    tts = {}
    for i in ids:
        tts[i] = datetime.fromisoformat(f'{d[0]:04}-{d[1]:02}-{rd.randint(1, 7):02}')
        if i not in dids:
            dids[i] = round(rd.uniform(10.0, 100.0), 2)
    for i in ids:
        tts[i] += timedelta(hours=rd.randint(3, 4), minutes=rd.randint(1, 30), seconds=rd.randint(1, 30)) 
        dids[i] = dids[i] + rd.choice([0, round(rd.uniform(5.0, 10.0), 2)])
        data.append((i, alphas[i%3]*3, dids[i], tts[i]))
    if dupf:
        for _ in range(int(len(ids)*(dupf+1))):
            i = rd.choice(ids)
            tts[i] += timedelta(days=rd.randint(5, 7), hours=rd.randint(3, 4), minutes=rd.randint(1, 30), seconds=rd.randint(1, 30))
            dids[i] = dids[i] + rd.choice([0, round(rd.uniform(5.0, 10.0), 2)])
            data.append((i, alphas[i%3]*3, dids[i], tts[i]))
    return spark.createDataFrame(data, schema)

def drop_table(db_name, table_name):
    spark.sql(f"drop table glue_catalog.{db_name}.{table_name};")

def show_table(db_name, table_name):
    spark.sql(f"select * from glue_catalog.{db_name}.{table_name};").show()

dids = {}
drop_table("xxxxxx", "ppbills")

schema = StructType([
    StructField("account_id", IntegerType()),
    StructField("category", StringType()),
    StructField("bill", FloatType()),
    StructField("recording_date", TimestampType())
])

db_name = "xxxxxx"
table_name = "ppbills"

query1 = f"""
CREATE TABLE {catalog_nm}.{db_name}.{table_name} (
    account_id int,
    category string,
    bill float,
    recording_date timestamp
)
USING iceberg
PARTITIONED BY (category)
-- LOCATION '{s3_bucket}{table_name}/' -- s3 permission required
TBLPROPERTIES ('format-version'='2');
"""
spark.sql(query1)

d24_1 = stream((2024, 1), range(1, 4), 0)

# inserting rows using dataframe
d24_1.write.insertInto(f"{catalog_nm}.{db_name}.{table_name}")

d24_2 = stream((2024, 1), range(4, 7), 0)
d24_2.createOrReplaceTempView("d24_2")

# inserting using tempview
spark.sql(
f"""
INSERT INTO {catalog_nm}.{db_name}.{table_name}
SELECT * FROM d24_2;
"""
)
show_table(db_name, table_name)

d24_3 = stream((2024, 3), range(5,9), 0)
d24_3.createOrReplaceTempView("d24_3")
d24_3.show()

spark.sql(
f"""
MERGE INTO {catalog_nm}.{db_name}.{table_name} as target
	USING d24_3 as source
    ON target.account_id = source.account_id
WHEN MATCHED AND 
	target.recording_date < source.recording_date 
	THEN UPDATE 
        SET 
	        target.bill = source.bill,
	        target.recording_date = source.recording_date
WHEN NOT MATCHED THEN
    INSERT (account_id, bill, recording_date, category)
    VALUES (
    source.account_id, source.bill,
    source.recording_date, source.category);
    """
)

spark.sql(
f"""
DELETE FROM {catalog_nm}.{db_name}.{table_name}
WHERE account_id > 5;
"""
)

df = spark.read.format("iceberg") \
	.load(f"{catalog_nm}.{db_name}.{table_name}")




spark.sql(f"select * from {catalog_nm}.{db_name}.{table_name}.snapshots").show()



spark.sql(
    f"""
    SELECT * FROM {catalog_nm}.{db_name}.{table_name}
    FOR VERSION AS OF 6962664543053544953
    """
).show()


%idle_timeout 60
%glue_version 4.0
%worker_type G.1X
%number_of_workers 2
%%configure
{
"--conf":"spark.serializer=org.apache.spark.serializer.KryoSerializer"
    " --conf spark.sql.hive.convertMetastoreParquet=false",
"--datalake-formats":"hudi"
}

from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.utils import getResolvedOptions
from pyspark.sql import SparkSession
s3_bucket = "s3://xxxxxx/prime/warehouse/"
sc = spark.sparkContext
glueContext = GlueContext(sc)
job = Job(glueContext)

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, TimestampType
import pyspark.sql.functions as f
from datetime import datetime, timedelta
import random as rd
from string import ascii_lowercase as alphas

def drop_table(db_name, table_name):
    spark.sql(f"drop table {db_name}.{table_name};")

def show_table(db_name, table_name):
    spark.sql(f"select * from {db_name}.{table_name};").show()

drop_table("xxxxxx", "pphbills")

def stream(d, ids, dupf=0):
    data = []
    tts = {}
    for i in ids:
        tts[i] = datetime.fromisoformat(f'{d[0]:04}-{d[1]:02}-{rd.randint(1, 7):02}')
        if i not in dids:
            dids[i] = round(rd.uniform(10.0, 100.0), 2)
    for i in ids:
        tts[i] += timedelta(hours=rd.randint(3, 4), minutes=rd.randint(1, 30), seconds=rd.randint(1, 30)) 
        dids[i] = dids[i] + rd.choice([0, round(rd.uniform(5.0, 10.0), 2)])
        data.append((i, dids[i], tts[i], alphas[i%3]*3))
    if dupf:
        for _ in range(int(len(ids)*(dupf+1))):
            i = rd.choice(ids)
            tts[i] += timedelta(days=rd.randint(5, 7), hours=rd.randint(3, 4), minutes=rd.randint(1, 30), seconds=rd.randint(1, 30))
            dids[i] = dids[i] + rd.choice([0, round(rd.uniform(5.0, 10.0), 2)])
            data.append((i, dids[i], tts[i], alphas[i%3]*3))
    return spark.createDataFrame(data, schema)

dids = {}

schema = StructType([
    StructField("account_id", IntegerType(), True),
    StructField("bill", FloatType(), True),
    StructField("timestamp", TimestampType(), True),
    StructField("category", StringType(), True),
])

db_name = "xxxxxx"
table_name = "pphbills"

additional_options={
    "hoodie.table.name": "pphbills",
    "hoodie.datasource.write.storage.type": "COPY_ON_WRITE",
    "hoodie.datasource.write.operation": "insert",
    "hoodie.datasource.write.recordkey.field": "account_id",
    "hoodie.datasource.write.precombine.field": "recording_date",
    "hoodie.datasource.write.partitionpath.field": "category",
    "hoodie.datasource.write.hive_style_partitioning": "true",
    "hoodie.datasource.hive_sync.enable": "true",
    "hoodie.datasource.hive_sync.database": f"{db_name}",
    "hoodie.datasource.hive_sync.table": f"{table_name}",
    # "hoodie.datasource.hive_sync.partition_fields": "partitionkey_field",
    "hoodie.datasource.hive_sync.partition_extractor_class": "org.apache.hudi.hive.MultiPartKeysValueExtractor",
    "hoodie.datasource.hive_sync.use_jdbc": "false",
    "hoodie.datasource.hive_sync.mode": "hms",
    "path": "s3://xxxxxx/prime/huditab/"
}

d24_1 = stream((2024, 1), range(1, 4), 0)
d24_1.show()
d24_1.write.format("hudi") \
    .options(**additional_options) \
    .mode("overwrite") \
    .save() # path provided in hudi options

d24_2 = stream((2024, 1), range(4, 6), 0)
d24_2.write.format("hudi") \
    .options(**additional_options) \
    .mode("append") \
    .save()

d24_3 = stream((2024, 3), range(4,8), 0)
d24_3.createOrReplaceTempView("d24_3")
d24_3.show()

additional_options["hoodie.datasource.write.operation"] = "upsert"
d24_3.write.format("hudi") \
    .options(**additional_options) \
    .mode("append") \
    .save()

delete_df = spark.sql(f"select * from {db_name}.{table_name} where account_id > 5;")

additional_options["hoodie.datasource.write.operation"] = "delete"
delete_df.write.format("hudi") \
    .options(**additional_options) \
    .mode("append") \
    .save()
