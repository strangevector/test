%idle_timeout 60
%glue_version 4.0
%worker_type G.1X
%number_of_workers 2
%%configure
{
"--conf":"spark.serializer=org.apache.spark.serializer.KryoSerializer"
    " --conf spark.sql.hive.convertMetastoreParquet=false",
"--datalake-formats":"hudi"
}

from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.utils import getResolvedOptions
from pyspark.sql import SparkSession
s3_bucket = "s3://duke-energy-optimized-substation-dashboard-e1-data-dev/stg/prime/warehouse/"
sc = spark.sparkContext
glueContext = GlueContext(sc)
job = Job(glueContext)

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, TimestampType
import pyspark.sql.functions as f
from datetime import datetime, timedelta
import random as rd
from string import ascii_lowercase as alphas

def drop_table(db_name, table_name):
    spark.sql(f"drop table {db_name}.{table_name};")

def show_table(db_name, table_name):
    spark.sql(f"select * from {db_name}.{table_name};").show()

drop_table("optimized_substation_dashboard_e1_stg_dev", "pphbills")

def stream(d, ids, dupf=0):
    data = []
    tts = {}
    for i in ids:
        tts[i] = datetime.fromisoformat(f'{d[0]:04}-{d[1]:02}-{rd.randint(1, 7):02}')
        if i not in dids:
            dids[i] = round(rd.uniform(10.0, 100.0), 2)
    for i in ids:
        tts[i] += timedelta(hours=rd.randint(3, 4), minutes=rd.randint(1, 30), seconds=rd.randint(1, 30)) 
        dids[i] = dids[i] + rd.choice([0, round(rd.uniform(5.0, 10.0), 2)])
        data.append((i, dids[i], tts[i], alphas[i%3]*3))
    if dupf:
        for _ in range(int(len(ids)*(dupf+1))):
            i = rd.choice(ids)
            tts[i] += timedelta(days=rd.randint(5, 7), hours=rd.randint(3, 4), minutes=rd.randint(1, 30), seconds=rd.randint(1, 30))
            dids[i] = dids[i] + rd.choice([0, round(rd.uniform(5.0, 10.0), 2)])
            data.append((i, dids[i], tts[i], alphas[i%3]*3))
    return spark.createDataFrame(data, schema)

dids = {}

schema = StructType([
    StructField("account_id", IntegerType(), True),
    StructField("bill", FloatType(), True),
    StructField("timestamp", TimestampType(), True),
    StructField("category", StringType(), True),
])

db_name = "optimized_substation_dashboard_e1_stg_dev"
table_name = "pphbills"

additional_options={
    "hoodie.table.name": "pphbills",
    "hoodie.datasource.write.storage.type": "COPY_ON_WRITE",
    "hoodie.datasource.write.operation": "insert",
    "hoodie.datasource.write.recordkey.field": "account_id",
    "hoodie.datasource.write.precombine.field": "recording_date",
    "hoodie.datasource.write.partitionpath.field": "category",
    "hoodie.datasource.write.hive_style_partitioning": "true",
    "hoodie.datasource.hive_sync.enable": "true",
    "hoodie.datasource.hive_sync.database": f"{db_name}",
    "hoodie.datasource.hive_sync.table": f"{table_name}",
    # "hoodie.datasource.hive_sync.partition_fields": "partitionkey_field",
    "hoodie.datasource.hive_sync.partition_extractor_class": "org.apache.hudi.hive.MultiPartKeysValueExtractor",
    "hoodie.datasource.hive_sync.use_jdbc": "false",
    "hoodie.datasource.hive_sync.mode": "hms",
    "path": "s3://duke-energy-optimized-substation-dashboard-e1-data-dev/stg/prime/huditab/"
}

d24_1 = stream((2024, 1), range(1, 4), 0)
d24_1.show()
d24_1.write.format("hudi") \
    .options(**additional_options) \
    .mode("overwrite") \
    .save() # path provided in hudi options

d24_2 = stream((2024, 1), range(4, 6), 0)
d24_2.write.format("hudi") \
    .options(**additional_options) \
    .mode("append") \
    .save()

d24_3 = stream((2024, 3), range(4,8), 0)
d24_3.createOrReplaceTempView("d24_3")
d24_3.show()


additional_options["hoodie.datasource.write.operation"] = "upsert"
d24_3.write.format("hudi") \
    .options(**additional_options) \
    .mode("append") \
    .save()

delete_df = spark.sql(f"select * from {db_name}.{table_name} where account_id > 5;")

additional_options["hoodie.datasource.write.operation"] = "delete"
delete_df.write.format("hudi") \
    .options(**additional_options) \
    .mode("append") \
    .save()

